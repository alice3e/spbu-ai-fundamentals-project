{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51729fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25117edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1820da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"all_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bdc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'experience_maps_to_save' not in globals():\n",
    "    experience_maps_to_save = {}\n",
    "if 'grouped_imputation_maps_to_save' not in globals():\n",
    "    grouped_imputation_maps_to_save = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54449dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4909342",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['worldwide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6622553",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_data = (data.isnull().mean() * 100).reset_index()\n",
    "nan_data.columns = [\"column_name\", \"percentage\"]\n",
    "nan_data.sort_values(\"percentage\", ascending=False, inplace=True)\n",
    "nan_data.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c442135",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['worldwide'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678cf945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "test_data.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b1b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_movie_title = train_data['movie_title']\n",
    "columns_to_drop = ['movie_id', 'movie_title', 'link']\n",
    "#columns_to_drop = ['movie_id', 'link']\n",
    "train_data = train_data.drop(columns=columns_to_drop)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ffbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. Heatmap корреляций числовых признаков\n",
    "# Выбираем только числовые колонки\n",
    "numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Считаем корреляции и сортируем по целевой переменной\n",
    "corr_matrix = train_data[numeric_cols].corr()\n",
    "target_corr = corr_matrix['worldwide'].sort_values(ascending=False)\n",
    "\n",
    "# Визуализируем топ-20 признаков по корреляции\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features = target_corr.index[1:21]  # исключаем сам worldwide\n",
    "sns.heatmap(train_data[top_features].corr(), \n",
    "            annot=True, fmt=\".2f\", \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            vmin=-1, vmax=1,\n",
    "            linewidths=0.5)\n",
    "plt.title('Тепловая карта корреляций (топ-20 признаков)', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Распределение целевой переменной\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Гистограмма\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train_data['worldwide'], kde=True, bins=50)\n",
    "plt.title(f'Распределение worldwide\\nSkewness: {train_data[\"worldwide\"].skew():.2f}')\n",
    "plt.xlabel('Цена ($)')\n",
    "plt.ylabel('Частота')\n",
    "\n",
    "# Boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=train_data['worldwide'])\n",
    "plt.title('Boxplot worldwide')\n",
    "plt.ylabel('Цена ($)')\n",
    "\n",
    "plt.suptitle('Анализ целевой переменной', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"budget\", \"domestic\", \"international\", \"worldwide\", \"run_time\"]\n",
    "\n",
    "for c in cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(train_data[c].dropna(), kde=True, stat=\"density\", bins=30)\n",
    "    plt.title(f\"Распределение и KDE для {c}\")\n",
    "    plt.xlabel(c)\n",
    "    plt.ylabel(\"Плотность\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46364ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(data=train_data[[\"domestic\", \"international\", \"worldwide\"]])\n",
    "plt.title(\"Boxplot: domestic vs international vs worldwide\")\n",
    "plt.ylabel(\"Сборы\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80876b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем колонку с декадой\n",
    "train_data[\"decade\"] = (train_data[\"movie_year\"] // 10) * 10\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"decade\", y=\"budget\", data=train_data)\n",
    "plt.title(\"Бюджет по десятилетиям выпуска\")\n",
    "plt.xlabel(\"Десятилетие\")\n",
    "plt.ylabel(\"Бюджет\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "train_data = train_data.drop(\"decade\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5762fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(\n",
    "    np.log1p(train_data[\"budget\"]),\n",
    "    np.log1p(train_data[\"worldwide\"]),\n",
    "    alpha=0.5\n",
    ")\n",
    "plt.title(\"log(budget) vs log(worldwide)\")\n",
    "plt.xlabel(\"log(Бюджет +1)\")\n",
    "plt.ylabel(\"log(Сборы worldwide +1)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f35b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=\"run_time\", y=\"worldwide\", data=train_data, alpha=0.6)\n",
    "plt.title(\"run_time vs worldwide\")\n",
    "plt.xlabel(\"run_time (мин)\")\n",
    "plt.ylabel(\"worldwide\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276088a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.regplot(x=\"movie_year\", y=\"worldwide\", data=train_data, scatter_kws={\"s\":30, \"alpha\":0.5})\n",
    "plt.title(\"Год выпуска vs worldwide (линейный тренд)\")\n",
    "plt.xlabel(\"Год выпуска\")\n",
    "plt.ylabel(\"worldwide\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in [\"genre_1\",\"genre_2\",\"genre_3\",\"genre_4\"]:\n",
    "    stats = (\n",
    "        train_data\n",
    "        .dropna(subset=[g])\n",
    "        .groupby(g)[\"worldwide\"]\n",
    "        .agg([\"count\",\"median\"])\n",
    "        .reset_index()\n",
    "        .sort_values(\"count\", ascending=False)\n",
    "    )\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=g, y=\"median\", data=stats, palette=\"viridis\")\n",
    "    plt.title(f\"Медианные сборы worldwide по {g} (частота показана числом над столбцами)\")\n",
    "    plt.xlabel(g)\n",
    "    plt.ylabel(\"Медианные сборы\")\n",
    "    for idx, row in stats.iterrows():\n",
    "        plt.text(idx, row[\"median\"] + 0.02*row[\"median\"], int(row[\"count\"]), \n",
    "                 ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# >>> ОБНОВЛЕННЫЙ БЛОК: Создание признаков опыта и СОХРАНЕНИЕ КАРТ <<<\n",
    "# ==============================================================================\n",
    "\n",
    "# Инициализируем словари для карт, которые мы хотим сохранить\n",
    "# Это нужно сделать один раз где-то в начале вашего скрипта предобработки,\n",
    "# если вы еще этого не сделали. Если уже сделали, эту строку можно пропустить.\n",
    "# Для примера, я добавлю инициализацию здесь, но лучше вынести ее выше.\n",
    "if 'experience_maps_to_save' not in globals(): # Проверяем, существует ли переменная\n",
    "    experience_maps_to_save = {}\n",
    "    print(\"Инициализирован 'experience_maps_to_save'\")\n",
    "\n",
    "personnel_cols_for_experience = [\"director\", \"writer\", \"producer\", \"composer\", \"cinematographer\"]\n",
    "print(\"\\nСоздание признаков опыта для съемочной группы и сохранение карт...\")\n",
    "for col in personnel_cols_for_experience:\n",
    "    if col in train_data.columns:\n",
    "        counts = train_data[col].value_counts()\n",
    "        # СОХРАНЯЕМ КАРТУ (словарь: имя -> количество)\n",
    "        experience_maps_to_save[f\"{col}_experience_map\"] = counts.to_dict()\n",
    "        \n",
    "        train_data[f\"{col}_experience\"] = train_data[col].map(counts)\n",
    "        # Заполняем пропуски нулями, если после map остались NaN \n",
    "        # (например, для значений, которых не было в train_data[col] при подсчете counts,\n",
    "        # или если train_data[col] имел NaN)\n",
    "        train_data[f\"{col}_experience\"].fillna(0, inplace=True)\n",
    "        print(f\"  Создан признак {col}_experience и сохранена карта.\")\n",
    "    else:\n",
    "        print(f\"  Предупреждение: столбец {col} не найден в train_data для создания опыта.\")\n",
    "\n",
    "# Опыт главных актёров и суммарная \"звёздность\"\n",
    "actor_experience_cols_generated = [] # Будем хранить имена созданных колонок опыта\n",
    "print(\"\\nСоздание признаков опыта для актеров и сохранение карт...\")\n",
    "for i in range(1, 5): # main_actor_1, main_actor_2, main_actor_3, main_actor_4\n",
    "    col_actor = f\"main_actor_{i}\"\n",
    "    if col_actor in train_data.columns:\n",
    "        counts_actor = train_data[col_actor].value_counts()\n",
    "        # СОХРАНЯЕМ КАРТУ\n",
    "        experience_maps_to_save[f\"{col_actor}_experience_map\"] = counts_actor.to_dict()\n",
    "        \n",
    "        experience_col_name = f\"{col_actor}_experience\"\n",
    "        train_data[experience_col_name] = train_data[col_actor].map(counts_actor)\n",
    "        train_data[experience_col_name].fillna(0, inplace=True)\n",
    "        actor_experience_cols_generated.append(experience_col_name)\n",
    "        print(f\"  Создан признак {experience_col_name} и сохранена карта.\")\n",
    "    else:\n",
    "        print(f\"  Предупреждение: столбец {col_actor} не найден для создания опыта.\")\n",
    "\n",
    "# Суммарная \"звёздность\"\n",
    "if actor_experience_cols_generated: # Только если были созданы колонки опыта актеров\n",
    "    train_data[\"cast_popularity\"] = train_data[actor_experience_cols_generated].sum(axis=1)\n",
    "    print(\"  Создан признак cast_popularity.\")\n",
    "else:\n",
    "    train_data[\"cast_popularity\"] = 0 \n",
    "    print(\"  Признак cast_popularity установлен в 0 (колонки опыта актеров не созданы).\")\n",
    "\n",
    "print(\"\\nСловарь experience_maps_to_save (пример):\")\n",
    "# Выведем первые несколько ключей для проверки\n",
    "for k in list(experience_maps_to_save.keys())[:2]:\n",
    "    print(f\"  {k}: {list(experience_maps_to_save[k].items())[:2]}...\") # Первые 2 элемента карты\n",
    "\n",
    "# ==============================================================================\n",
    "# >>> КОНЕЦ ОБНОВЛЕННОГО БЛОКА <<<\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e44a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_runtime_to_minutes(value):\n",
    "    match = re.match(r'(?:(\\d+)\\s*hr)?\\s*(?:(\\d+)\\s*min)?', str(value))\n",
    "    if match:\n",
    "        hours = int(match.group(1)) if match.group(1) else 0\n",
    "        minutes = int(match.group(2)) if match.group(2) else 0\n",
    "        return hours * 60 + minutes\n",
    "    return None  # если формат не распознан\n",
    "\n",
    "train_data['run_time'] = train_data['run_time'].apply(convert_runtime_to_minutes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numerical_cols_for_imputation_training = [\n",
    "    col for col in train_data.select_dtypes(include=[np.number]).columns \n",
    "    if col not in ['worldwide', 'domestic', 'international'] # Исключаем!\n",
    "]\n",
    "if numerical_cols_for_imputation_training: # Только если есть что импьютировать\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    train_data[numerical_cols_for_imputation_training] = \\\n",
    "        num_imputer.fit_transform(train_data[numerical_cols_for_imputation_training])\n",
    "    print(f\"Numerical Imputer обучен на колонках: {num_imputer.feature_names_in_.tolist()}\")\n",
    "else:\n",
    "    num_imputer = None # На случай, если нет числовых предикторов\n",
    "    print(\"ПРЕДУПРЕЖДЕНИЕ: Нет числовых колонок для обучения Numerical Imputer (после исключения target-related).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b59d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_cols = train_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_data[num_cols] = imputer.fit_transform(train_data[num_cols])\n",
    "train_data.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a744f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['director']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_data = (train_data.isnull().mean() * 100).reset_index()\n",
    "nan_data.columns = [\"column_name\", \"percentage\"]\n",
    "nan_data.sort_values(\"percentage\", ascending=False, inplace=True)\n",
    "nan_data.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_cols = [\"genre_1\", \"genre_2\", \"genre_3\", \"genre_4\"]\n",
    "# число жанров\n",
    "'''train_data[\"num_genres\"] = train_data[genre_cols].notna().sum(axis=1)\n",
    "genres = train_data[genre_cols].apply(lambda row: [g for g in row if pd.notna(g)], axis=1)\n",
    "family_genres = {\"Animation\", \"Family\", \"Adventure\"}\n",
    "train_data[\"is_family_friendly\"] = genres.apply(lambda gl: int(bool(set(gl) & family_genres)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd29d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# >>> ОБНОВЛЕННЫЙ БЛОК: Групповая импутация и СОХРАНЕНИЕ КАРТ МОД <<<\n",
    "# ==============================================================================\n",
    "\n",
    "cols_for_grouped_imputation = ['cinematographer', 'composer', 'producer', 'writer'] # Переименовал для ясности\n",
    "\n",
    "# Инициализируем словарь для карт мод, если еще не сделано\n",
    "if 'grouped_imputation_maps_to_save' not in globals():\n",
    "    grouped_imputation_maps_to_save = {}\n",
    "    print(\"Инициализирован 'grouped_imputation_maps_to_save'\")\n",
    "\n",
    "print(\"\\nГрупповая импутация (модой по 'director') и сохранение карт мод...\")\n",
    "if 'director' in train_data.columns:\n",
    "    for col_to_impute in cols_for_grouped_imputation:\n",
    "        if col_to_impute in train_data.columns:\n",
    "            print(f\"  Обработка колонки '{col_to_impute}'...\")\n",
    "            \n",
    "            # 1. СОХРАНЯЕМ КАРТУ: director -> mode_of_col_to_impute\n",
    "            # Эта карта будет содержать для каждого режиссера наиболее частый cinematographer/composer и т.д.\n",
    "            # np.nan будет, если для режиссера все значения в col_to_impute были NaN или группа пуста\n",
    "            director_to_mode_map = train_data.groupby('director')[col_to_impute].apply(\n",
    "                lambda x: x.mode().iloc[0] if not x.mode().empty and not x.mode().isnull().all() else np.nan\n",
    "            )\n",
    "            grouped_imputation_maps_to_save[f\"{col_to_impute}_director_mode_map\"] = director_to_mode_map.to_dict()\n",
    "            print(f\"    Карта мод для '{col_to_impute}' по 'director' сохранена.\")\n",
    "            # print(f\"    Пример карты для {col_to_impute}: {list(director_to_mode_map.items())[:2]}\") # Для отладки\n",
    "\n",
    "            # 2. ПРИМЕНЯЕМ ИМПУТАЦИЮ, используя созданную карту\n",
    "            # Сначала получаем серию мод для каждого значения в 'director' текущей строки\n",
    "            # Если режиссера нет в карте (например, новый режиссер в тестовых данных), map вернет NaN\n",
    "            mapped_modes_for_imputation = train_data['director'].map(director_to_mode_map)\n",
    "            \n",
    "            # Заполняем пропуски в col_to_impute значениями из mapped_modes_for_imputation\n",
    "            train_data[col_to_impute].fillna(mapped_modes_for_imputation, inplace=True)\n",
    "            print(f\"    Пропуски в '{col_to_impute}' заполнены с использованием сохраненной карты мод.\")\n",
    "        else:\n",
    "            print(f\"  Предупреждение: столбец '{col_to_impute}' не найден для групповой импутации.\")\n",
    "else:\n",
    "    print(\"  Предупреждение: столбец 'director' не найден, групповая импутация не будет выполнена.\")\n",
    "\n",
    "print(\"\\nСловарь grouped_imputation_maps_to_save (пример):\")\n",
    "for k in list(grouped_imputation_maps_to_save.keys())[:2]:\n",
    "    print(f\"  {k}: {list(grouped_imputation_maps_to_save[k].items())[:2]}...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# >>> КОНЕЦ ОБНОВЛЕННОГО БЛОКА <<<\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639828d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill = ['genre_2', 'genre_3', 'genre_4', 'main_actor_4']\n",
    "train_data[columns_to_fill] = train_data[columns_to_fill].fillna('Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957bde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb0eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "train_data[cat_cols] = cat_imputer.fit_transform(train_data[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce9d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_data = (train_data.isnull().mean() * 100).reset_index()\n",
    "nan_data.columns = [\"column_name\", \"percentage\"]\n",
    "nan_data.sort_values(\"percentage\", ascending=False, inplace=True)\n",
    "nan_data.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc88d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['run_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be6d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import re\n",
    "\n",
    "def convert_runtime_to_minutes(value):\n",
    "    match = re.match(r'(?:(\\d+)\\s*hr)?\\s*(?:(\\d+)\\s*min)?', str(value))\n",
    "    if match:\n",
    "        hours = int(match.group(1)) if match.group(1) else 0\n",
    "        minutes = int(match.group(2)) if match.group(2) else 0\n",
    "        return hours * 60 + minutes\n",
    "    return None  # если формат не распознан\n",
    "\n",
    "train_data['run_time'] = train_data['run_time'].apply(convert_runtime_to_minutes)\n",
    "train_data.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7806a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод столбцов и колисество значений с ними\n",
    "cat_cols = train_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "cat_stats = pd.DataFrame({\n",
    "    'Column': cat_cols,\n",
    "    'Unique_Count': [train_data[col].nunique() for col in cat_cols],\n",
    "    'Unique_Values': [train_data[col].unique() for col in cat_cols]\n",
    "})\n",
    "\n",
    "cat_stats = cat_stats.sort_values(by='Unique_Count', ascending=False)\n",
    "print(cat_stats[['Column', 'Unique_Count']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_target_encoding = ['main_actor_4', 'main_actor_3', 'writer', 'main_actor_2', 'producer', 'director', 'main_actor_1', 'cinematographer', 'composer', 'distributor']\n",
    "columns_for_one_hot = ['genre_2', 'genre_3', 'genre_4', 'genre_1', 'mpaa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5277c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "ohe.fit(train_data[columns_for_one_hot])\n",
    "\n",
    "# Трансформируем трейн и тест\n",
    "train_encoded = ohe.transform(train_data[columns_for_one_hot])\n",
    "#train_data = pd.get_dummies(train_data, columns=columns_for_one_hot)\n",
    "train_encoded_df = pd.DataFrame(\n",
    "    train_encoded,\n",
    "    columns=ohe.get_feature_names_out(columns_for_one_hot),\n",
    "    index=train_data.index\n",
    ")\n",
    "train_data = train_data.drop(columns_for_one_hot, axis=1).join(train_encoded_df)\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "encoder = ce.TargetEncoder(cols=columns_for_target_encoding)\n",
    "train_data_encoded = encoder.fit_transform(train_data[columns_for_target_encoding], train_data['worldwide'])\n",
    "train_data[columns_for_target_encoding] = train_data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e579639",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(\"worldwide\", axis=1)\n",
    "y = train_data[\"worldwide\"]\n",
    "#y = np.log1p(train_data[\"worldwide\"])\n",
    "\n",
    "cols_to_drop_for_X = ['worldwide']\n",
    "if 'domestic' in train_data.columns and 'domestic' not in X.columns: # Если domestic не должен быть в X\n",
    "    cols_to_drop_for_X.append('domestic')\n",
    "if 'international' in train_data.columns and 'international' not in X.columns: # Если international не должен быть в X\n",
    "    cols_to_drop_for_X.append('international')\n",
    "\n",
    "# Убедимся, что удаляем только существующие\n",
    "actual_cols_to_drop_for_X = [col for col in cols_to_drop_for_X if col in train_data.columns]\n",
    "\n",
    "X = train_data.drop(columns=actual_cols_to_drop_for_X)\n",
    "y = train_data[\"worldwide\"]\n",
    "print(f\"Колонки в X (финальные предикторы, первые 10): {X.columns.tolist()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fcfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a49839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler,MaxAbsScaler, QuantileTransformer, PowerTransformer\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e459ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "'''model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(f\"R²: {r2_score(y_val, y_pred):.4f}\") #R²: 0.9981\n",
    "print(f\"MSE: {mean_squared_error(y_val, y_pred):.2f}\")#MSE: 73899943426108.95'''\n",
    "\n",
    "\n",
    "\n",
    "'''rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Сетка гиперпараметров\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 1000],           # Кол-во деревьев\n",
    "    'max_depth': [10, 50, None],          # Максимальная глубина\n",
    "    'min_samples_split': [2, 10],          # Мин. объектов для разделения\n",
    "    'min_samples_leaf': [1, 5],           # Мин. объектов в листе\n",
    "    'max_features': ['auto', 'sqrt']      # Кол-во признаков при делении\n",
    "}\n",
    "\n",
    "# GridSearch с кросс-валидацией\n",
    "grid_search = GridSearchCV(estimator=rf,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=3,\n",
    "                           scoring='r2',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# Обучение модели\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшая модель и параметры\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Предсказания и метрики\n",
    "y_pred = best_model.predict(X_val)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "print(f\"Test R² Score: {r2:.4f}\") # Test R² Score: 0.9801\n",
    "print(f\"Test MSE: {mse:.2f}\") #Test MSE: 756709767970853.50'''\n",
    "\n",
    "\n",
    "\n",
    "'''from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(n_estimators=500, learning_rate=0.1, max_depth=8, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(f\"R²: {r2_score(y_val, y_pred):.4f}\")#R²: 0.9984\n",
    "print(f\"MSE: {mean_squared_error(y_val, y_pred):.2f}\")#MSE: 61650183675109.39'''\n",
    "\n",
    "\n",
    "\n",
    "'''from lightgbm import LGBMRegressor\n",
    "\n",
    "model = LGBMRegressor(n_estimators=1000, learning_rate=0.21, max_depth=-1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(f\"R²: {r2_score(y_val, y_pred):.4f}\") # R²: 0.9953\n",
    "print(f\"MSE: {mean_squared_error(y_val, y_pred):.2f}\") #MSE: 180661829545789.47'''\n",
    "\n",
    "\n",
    "\n",
    "'''from catboost import CatBoostRegressor\n",
    "\n",
    "model = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, verbose=0, random_seed=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(f\"R²: {r2_score(y_val, y_pred):.4f}\") # MSE: 145843899870827.59\n",
    "print(f\"MSE: {mean_squared_error(y_val, y_pred):.2f}\") # R²: 0.9962'''\n",
    "\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth':    [4, 6, 8],\n",
    "    'learning_rate':[0.01, 0.05, 0.1],\n",
    "    'subsample':    [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha':    [0, 0.5, 1],\n",
    "    'reg_lambda':   [1, 2, 5]\n",
    "}\n",
    "\n",
    "# 4) Создаём XGB с фиксированным seed и детерминированным методом\n",
    "base_model = XGBRegressor(\n",
    "    random_state=42,\n",
    "    tree_method='hist',            # более детерминированный\n",
    "    enable_categorical=False,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# 5) RandomizedSearchCV с фиксированным seed\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=1,              # лучше 1, чтобы исключить недетерминированность\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6) Обучаем\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 7) Оцениваем\n",
    "best = random_search.best_estimator_\n",
    "y_pred = best.predict(X_val)\n",
    "print(f\"R²:  {r2_score(y_val, y_pred):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_val, y_pred):.2f}\")\n",
    "#Best Params: {'subsample': 0.8, 'reg_lambda': 5, 'reg_alpha': 1, 'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
    "#Best R² Score on Test: 0.9990\n",
    "#Best MSE on Test: 37542782216082.52\n",
    "\n",
    "#Best R² Score on Test: 0.9990\n",
    "#Best MSE on Test: 36496695202244.70\n",
    "\n",
    "#Best R² Score on Test: 0.9991\n",
    "#Best MSE on Test: 32663862493933.65\n",
    "\n",
    "\n",
    "#R²:  0.9993\n",
    "#MSE: 28215564467164.59\n",
    "\n",
    "\n",
    "'''param_dist = {\n",
    "    'iterations': [200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'random_strength': [0.5, 1, 2]\n",
    "}\n",
    "\n",
    "# Создаём модель\n",
    "cat = CatBoostRegressor(verbose=0, random_seed=42)\n",
    "\n",
    "# RandomizedSearch\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=cat,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшая модель\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Предсказания и метрики\n",
    "y_pred = best_model.predict(X_val)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "print(f\"Best R²: {r2:.4f}\")\n",
    "print(f\"Best MSE: {mse:.2f}\")\n",
    "#Best Parameters: {'subsample': 0.8, 'random_strength': 0.5, 'learning_rate': 0.05, 'l2_leaf_reg': 5, 'iterations': 300, 'depth': 6}\n",
    "#Best R²: 0.9978\n",
    "#Best MSE: 83318573388615.64 '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''param_dist = {\n",
    "    'n_estimators': [200, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [-1, 4, 6, 8, 10],\n",
    "    'num_leaves': [31, 50, 100, 150],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Модель\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшая модель и параметры\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Предсказание и метрики\n",
    "y_pred = best_model.predict(X_val)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "print(f\"Best R²: {r2:.4f}\")\n",
    "print(f\"Best MSE: {mse:.2f}\")\n",
    "#Best Parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0.1, 'num_leaves': 31, 'n_estimators': 1000, 'max_depth': 4, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
    "#Best R²: 0.9959\n",
    "#Best MSE: 157899903576786.03'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''param_dist = {\n",
    "    'n_estimators': [200, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [-1, 4, 6, 8, 10],\n",
    "    'num_leaves': [31, 50, 100, 150],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Модель\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Randomized Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшая модель и параметры\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Предсказание и метрики\n",
    "y_pred = best_model.predict(X_val)# Обратное преобразование: экспоненцируем предсказания\n",
    "y_pred_original = np.expm1(y_pred)  # Возвращаем обратно в исходный масштаб\n",
    "\n",
    "# Метрики\n",
    "r2 = r2_score(np.expm1(y_val), y_pred_original)  # Обратно логарифмируем y_val для расчета метрик\n",
    "mse = mean_squared_error(np.expm1(y_val), y_pred_original)\n",
    "print(f\"Best R²: {r2:.4f}\")\n",
    "print(f\"Best MSE: {mse:.2f}\")\n",
    "#Best R²: 0.9981\n",
    "#Best MSE: 73636379429094.73'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''param_dist = {\n",
    "    'iterations': [200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'random_strength': [0.5, 1, 2]\n",
    "}\n",
    "\n",
    "# Создаём модель\n",
    "cat = CatBoostRegressor(verbose=0, random_seed=42)\n",
    "\n",
    "# RandomizedSearch\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=cat,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшая модель\n",
    "best_model = random_search.best_estimator_\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Предсказания и метрики\n",
    "y_pred = best_model.predict(X_val)\n",
    "y_pred_original = np.expm1(y_pred)  # Возвращаем обратно в исходный масштаб\n",
    "\n",
    "# Метрики\n",
    "r2 = r2_score(np.expm1(y_val), y_pred_original)  # Обратно логарифмируем y_val для расчета метрик\n",
    "mse = mean_squared_error(np.expm1(y_val), y_pred_original)\n",
    "\n",
    "print(f\"Best R²: {r2:.4f}\")\n",
    "print(f\"Best MSE: {mse:.2f}\")\n",
    "#Best R²: 0.9934\n",
    "#Best MSE: 252026444906922.81'''\n",
    "\n",
    "\n",
    "\n",
    "'''param_dist = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.5, 1],\n",
    "    'reg_lambda': [1, 2, 5]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best Params:\", random_search.best_params_)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Лучшая модель из RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Предсказания на тестовой выборке\n",
    "y_pred = best_model.predict(X_val)\n",
    "y_pred_original = np.expm1(y_pred)  # Возвращаем обратно в исходный масштаб\n",
    "\n",
    "# Метрики\n",
    "r2 = r2_score(np.expm1(y_val), y_pred_original)  # Обратно логарифмируем y_val для расчета метрик\n",
    "mse = mean_squared_error(np.expm1(y_val), y_pred_original)\n",
    "\n",
    "print(f\"Best R² Score on Test: {r2:.4f}\")\n",
    "print(f\"Best MSE on Test: {mse:.2f}\")\n",
    "#Best R² Score on Test: 0.9975\n",
    "#Best MSE on Test: 95385734348795.59'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Сетка гиперпараметров\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 1000],           # Кол-во деревьев\n",
    "    'max_depth': [10, 50, None],          # Максимальная глубина\n",
    "    'min_samples_split': [2, 10],          # Мин. объектов для разделения\n",
    "    'min_samples_leaf': [1, 5],           # Мин. объектов в листе\n",
    "    'max_features': ['auto', 'sqrt']      # Кол-во признаков при делении\n",
    "}\n",
    "\n",
    "# GridSearch с кросс-валидацией\n",
    "grid_search = GridSearchCV(estimator=rf,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=3,\n",
    "                           scoring='r2',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# Обучение модели\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Лучшая модель и параметры\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Предсказания и метрики\n",
    "y_pred = best_model.predict(X_val)\n",
    "y_pred_original = np.expm1(y_pred)  # Возвращаем обратно в исходный масштаб\n",
    "\n",
    "# Метрики\n",
    "r2 = r2_score(np.expm1(y_val), y_pred_original)  # Обратно логарифмируем y_val для расчета метрик\n",
    "mse = mean_squared_error(np.expm1(y_val), y_pred_original)\n",
    "\n",
    "print(f\"Test R² Score: {r2:.4f}\")\n",
    "print(f\"Test MSE: {mse:.2f}\") \n",
    "#Test R² Score: 0.9648\n",
    "#Test MSE: 1341694408103681.00'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b9d15",
   "metadata": {},
   "source": [
    "TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff60ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_movie_title = test_data['movie_title']\n",
    "columns_to_drop = ['movie_id', 'movie_title', 'link']\n",
    "#columns_to_drop = ['movie_id', 'link']\n",
    "test_data = test_data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4516869",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"director\", \"writer\", \"producer\", \"composer\", \"cinematographer\"]:\n",
    "    counts = train_data[col].value_counts()\n",
    "    test_data[f\"{col}_experience\"] = test_data[col].map(counts).fillna(0)\n",
    "for i in range(1, 5):\n",
    "    col = f\"main_actor_{i}\"\n",
    "    counts = train_data[col].value_counts()\n",
    "    test_data[f\"{col}_experience\"] = test_data[col].map(counts).fillna(0)\n",
    "\n",
    "test_data[\"cast_popularity\"] = sum(test_data[f\"main_actor_{i}_experience\"] for i in range(1, 5))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ea6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_runtime_to_minutes(value):\n",
    "    match = re.match(r'(?:(\\d+)\\s*hr)?\\s*(?:(\\d+)\\s*min)?', str(value))\n",
    "    if match:\n",
    "        hours = int(match.group(1)) if match.group(1) else 0\n",
    "        minutes = int(match.group(2)) if match.group(2) else 0\n",
    "        return hours * 60 + minutes\n",
    "    return None  # если формат не распознан\n",
    "\n",
    "test_data['run_time'] = test_data['run_time'].apply(convert_runtime_to_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baceadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num_cols = test_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "print(test_num_cols)\n",
    "test_data[test_num_cols] = num_imputer.transform(test_data[test_num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07009ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_fill = ['cinematographer', 'composer', 'producer', 'writer']\n",
    "\n",
    "for col in cols_to_fill:\n",
    "    mode_values = train_data.groupby('director')[col].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "    test_data[col] = test_data.apply(lambda row: mode_values.get(row['director'], np.nan) if pd.isnull(row[col]) else row[col], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a740ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill = ['genre_2', 'genre_3', 'genre_4', 'main_actor_4']\n",
    "test_data[columns_to_fill] = test_data[columns_to_fill].fillna('Unknown')\n",
    "\n",
    "cat_cols = test_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "test_data[cat_cols] = cat_imputer.fit_transform(test_data[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5320b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_data = (test_data.isnull().mean() * 100).reset_index()\n",
    "nan_data.columns = [\"column_name\", \"percentage\"]\n",
    "nan_data.sort_values(\"percentage\", ascending=False, inplace=True)\n",
    "nan_data.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58064670",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = ohe.transform(test_data[columns_for_one_hot])\n",
    "test_encoded_df = pd.DataFrame(\n",
    "    test_encoded,\n",
    "    columns=ohe.get_feature_names_out(columns_for_one_hot),\n",
    "    index=test_data.index\n",
    ")\n",
    "\n",
    "test_data = test_data.drop(columns_for_one_hot, axis=1).join(test_encoded_df)\n",
    "test_data = test_data.reindex(columns=train_data.columns, fill_value=0)\n",
    "test_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_encoded = encoder.fit_transform(test_data[columns_for_target_encoding], test_data['worldwide'])\n",
    "test_data[columns_for_target_encoding] = test_data_encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09f49ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test =test_data.drop(\"worldwide\", axis=1)\n",
    "y_test = test_data[\"worldwide\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610082aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best.predict(X_test)\n",
    "print(f\"R²:  {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# >>> ОБНОВЛЕННЫЙ БЛОК СОХРАНЕНИЯ АРТЕФАКТОВ <<<\n",
    "# Использует experience_maps_to_save и grouped_imputation_maps_to_save,\n",
    "# которые должны быть определены и заполнены ранее в ноутбуке.\n",
    "# ==============================================================================\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import pandas as pd # Убедимся, что pandas импортирован для pd.isna\n",
    "import numpy as np  # Убедимся, что numpy импортирован для np.nan\n",
    "\n",
    "# --- 0. Определяем директорию для сохранения ---\n",
    "MODEL_SAVE_DIR = \"saved_model\" \n",
    "if not os.path.exists(MODEL_SAVE_DIR):\n",
    "    os.makedirs(MODEL_SAVE_DIR)\n",
    "print(f\"Артефакты будут сохранены в директорию: {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# --- 1. Сохранение обученной модели ---\n",
    "# Ваш 'best' - это XGBRegressor (обертка sklearn)\n",
    "# Если вы хотите сохранить его как XGBRegressor, используйте joblib\n",
    "model_filename_joblib = os.path.join(MODEL_SAVE_DIR, \"movie_box_office_model.joblib\")\n",
    "joblib.dump(best, model_filename_joblib)\n",
    "print(f\"Модель (XGBRegressor) сохранена как: {model_filename_joblib}\")\n",
    "\n",
    "# Если вы хотите сохранить XGBoost Booster в JSON формате (как в вашем tree для predictor.py):\n",
    "# Убедитесь, что 'best' это действительно XGBoost модель, а не, например, Pipeline\n",
    "if hasattr(best, 'save_model'): # Проверка, есть ли у объекта метод save_model (как у XGBoost Booster/Regressor)\n",
    "    model_filename_json = os.path.join(MODEL_SAVE_DIR, \"movie_box_office_model.json\")\n",
    "    try:\n",
    "        # Если 'best' это scikit-learn обертка XGBRegressor, то get_booster() вернет \"сырой\" Booster\n",
    "        booster_to_save = best.get_booster() if hasattr(best, 'get_booster') else best\n",
    "        booster_to_save.save_model(model_filename_json)\n",
    "        print(f\"Модель XGBoost (Booster) также сохранена в JSON формате: {model_filename_json}\")\n",
    "    except Exception as e_save_json:\n",
    "        print(f\"Не удалось сохранить модель в JSON формате: {e_save_json}. Используйте .joblib версию.\")\n",
    "else:\n",
    "    print(\"Модель 'best' не имеет метода save_model, JSON версия не сохранена.\")\n",
    "\n",
    "\n",
    "# --- 2. Сохранение трансформеров ---\n",
    "joblib.dump(num_imputer, os.path.join(MODEL_SAVE_DIR, \"numerical_imputer.joblib\"))\n",
    "print(\"Numerical Imputer сохранен.\")\n",
    "\n",
    "joblib.dump(cat_imputer, os.path.join(MODEL_SAVE_DIR, \"categorical_imputer.joblib\"))\n",
    "print(\"Categorical Imputer сохранен.\")\n",
    "\n",
    "joblib.dump(ohe, os.path.join(MODEL_SAVE_DIR, \"onehot_encoder.joblib\")) # ohe - ваш sklearn.preprocessing.OneHotEncoder\n",
    "print(\"OneHotEncoder сохранен.\")\n",
    "\n",
    "joblib.dump(encoder, os.path.join(MODEL_SAVE_DIR, \"target_encoder.joblib\")) # encoder - ваш ce.TargetEncoder\n",
    "print(\"Target Encoder сохранен.\")\n",
    "\n",
    "joblib.dump(scaler, os.path.join(MODEL_SAVE_DIR, \"scaler.joblib\")) # scaler - ваш StandardScaler\n",
    "print(\"Scaler сохранен.\")\n",
    "\n",
    "\n",
    "# --- 3. Сохранение информации о колонках и карт преобразований ---\n",
    "\n",
    "# Списки колонок, определенные в вашем ноутбуке:\n",
    "# columns_to_drop - должен быть определен ранее, например: ['movie_id', 'movie_title', 'link']\n",
    "# columns_for_one_hot - должен быть определен ранее, например: ['genre_2', 'genre_3', 'genre_4', 'genre_1', 'mpaa']\n",
    "# columns_for_target_encoding - должен быть определен ранее\n",
    "\n",
    "# Финальные признаки для модели (X.columns.tolist() ПОСЛЕ OHE и Target Encoding, ДО scaling)\n",
    "# Убедитесь, что DataFrame 'X' существует на этом этапе и содержит правильные колонки\n",
    "if 'X' not in globals() or not isinstance(X, pd.DataFrame):\n",
    "    raise NameError(\"DataFrame 'X' с финальными признаками не определен перед сохранением column_info.\")\n",
    "final_model_features_list = X.columns.tolist()\n",
    "\n",
    "# Колонки, на которых обучался numerical_imputer\n",
    "# Атрибут feature_names_in_ хранит имена колонок, на которых был сделан fit\n",
    "if not hasattr(num_imputer, 'feature_names_in_'):\n",
    "    raise AttributeError(\"Атрибут 'feature_names_in_' отсутствует у num_imputer. Убедитесь, что импьютер был обучен.\")\n",
    "numerical_imputer_cols_fitted_on = num_imputer.feature_names_in_.tolist()\n",
    "# ВАЖНО: Если 'domestic', 'international', 'worldwide' НЕ являются предикторами,\n",
    "# они не должны быть в этом списке. Это значит, что num_imputer должен был обучаться\n",
    "# на DataFrame, из которого эти колонки уже удалены (если они не предикторы).\n",
    "# Если они попали в fit, predictor.py должен будет их ожидать.\n",
    "# Рекомендуется обучать num_imputer только на тех числовых колонках, которые являются предикторами и могут иметь пропуски.\n",
    "\n",
    "\n",
    "# Колонки, на которых обучался categorical_imputer\n",
    "if not hasattr(cat_imputer, 'feature_names_in_'):\n",
    "    raise AttributeError(\"Атрибут 'feature_names_in_' отсутствует у cat_imputer.\")\n",
    "categorical_imputer_cols_fitted_on = cat_imputer.feature_names_in_.tolist()\n",
    "\n",
    "# Колонки, на которые подавался OneHotEncoder (исходные)\n",
    "# columns_for_one_hot - это ваш список исходных колонок для OHE\n",
    "ohe_input_columns_list = columns_for_one_hot\n",
    "\n",
    "# Колонки для TargetEncoder (исходные)\n",
    "# columns_for_target_encoding - ваш список\n",
    "target_encoding_columns_list = columns_for_target_encoding\n",
    "\n",
    "\n",
    "# Используем experience_maps_to_save и grouped_imputation_maps_to_save,\n",
    "# которые вы создали и заполнили ранее в ноутбуке.\n",
    "if 'experience_maps_to_save' not in globals() or 'grouped_imputation_maps_to_save' not in globals():\n",
    "    print(\"ПРЕДУПРЕЖДЕНИЕ: 'experience_maps_to_save' или 'grouped_imputation_maps_to_save' не найдены.\")\n",
    "    print(\"              Карты опыта и/или групповой импутации могут быть не сохранены или сохранены пустыми.\")\n",
    "    # Создаем пустые, чтобы код не упал, но это будет некорректно для predictor.py\n",
    "    experience_maps_to_save = experience_maps_to_save if 'experience_maps_to_save' in globals() else {}\n",
    "    grouped_imputation_maps_to_save = grouped_imputation_maps_to_save if 'grouped_imputation_maps_to_save' in globals() else {}\n",
    "\n",
    "\n",
    "column_info_data = {\n",
    "    \"features\": final_model_features_list, # Финальные признаки для модели\n",
    "    \"target\": \"worldwide\",                 # Имя целевой переменной\n",
    "    \n",
    "    \"initial_columns_to_drop\": columns_to_drop, # Начальные колонки для удаления\n",
    "    \"ohe_input_columns\": ohe_input_columns_list, # Исходные колонки для OneHotEncoder\n",
    "    \"target_encoding_columns\": target_encoding_columns_list, # Исходные колонки для TargetEncoder\n",
    "\n",
    "    \"numerical_imputer_features_in\": numerical_imputer_cols_fitted_on,\n",
    "    \"categorical_imputer_features_in\": categorical_imputer_cols_fitted_on,\n",
    "\n",
    "    \"experience_maps\": experience_maps_to_save, \n",
    "    \"grouped_imputation_maps\": grouped_imputation_maps_to_save,\n",
    "    \n",
    "    # Списки колонок, которые могут помочь predictor.py понять логику вашего ноутбука\n",
    "    \"personnel_cols_for_experience\": [\"director\", \"writer\", \"producer\", \"composer\", \"cinematographer\"],\n",
    "    \"actor_cols_for_experience_prefix\": \"main_actor_\",\n",
    "    # Колонки, которые импьютировались модой по 'director'\n",
    "    \"cols_for_grouped_imputation_source_director\": ['cinematographer', 'composer', 'producer', 'writer'], # Из вашего кода\n",
    "    # Колонки, которые заполнялись 'Unknown'\n",
    "    \"cols_to_fill_unknown_specific\": ['genre_2', 'genre_3', 'genre_4', 'main_actor_4'] # Из вашего кода\n",
    "}\n",
    "\n",
    "# Очистка np.nan для JSON сериализации в картах\n",
    "def clean_map_for_json(data_map):\n",
    "    if not isinstance(data_map, dict): return {} # Если не словарь, возвращаем пустой\n",
    "    cleaned = {}\n",
    "    for map_key, inner_dict in data_map.items():\n",
    "        if isinstance(inner_dict, dict):\n",
    "             cleaned[map_key] = {str(k): (None if pd.isna(v) else v) for k, v in inner_dict.items()}\n",
    "        else: # Если значение в data_map не словарь (маловероятно для карт, но для безопасности)\n",
    "            cleaned[map_key] = None if pd.isna(inner_dict) else inner_dict \n",
    "    return cleaned\n",
    "\n",
    "# Применяем очистку только если карты не пустые\n",
    "if column_info_data[\"experience_maps\"]:\n",
    "    column_info_data[\"experience_maps\"] = clean_map_for_json(column_info_data[\"experience_maps\"])\n",
    "if column_info_data[\"grouped_imputation_maps\"]:\n",
    "    column_info_data[\"grouped_imputation_maps\"] = clean_map_for_json(column_info_data[\"grouped_imputation_maps\"])\n",
    "\n",
    "\n",
    "column_info_filename = os.path.join(MODEL_SAVE_DIR, \"column_info.json\")\n",
    "with open(column_info_filename, \"w\", encoding='utf-8') as f:\n",
    "    json.dump(column_info_data, f, indent=2, ensure_ascii=False)\n",
    "print(f\"Информация о колонках сохранена как: {column_info_filename}\")\n",
    "\n",
    "# --- 4. (Опционально) Сохранение all_data.csv, если generate_options.py его оттуда читает ---\n",
    "# import shutil\n",
    "# source_all_data = \"all_data.csv\" # Укажите правильный путь к вашему all_data.csv\n",
    "# if os.path.exists(source_all_data):\n",
    "#    dest_all_data = os.path.join(MODEL_SAVE_DIR, \"all_data.csv\")\n",
    "#    shutil.copyfile(source_all_data, dest_all_data)\n",
    "#    print(f\"Файл {source_all_data} скопирован в {dest_all_data}\")\n",
    "# else:\n",
    "#    print(f\"ПРЕДУПРЕЖДЕНИЕ: Исходный файл {source_all_data} не найден, не скопирован.\")\n",
    "\n",
    "print(\">>> СОХРАНЕНИЕ АРТЕФАКТОВ ЗАВЕРШЕНО <<<\")\n",
    "# =============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
